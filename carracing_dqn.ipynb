{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on and partly taken from https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Valentin\\AppData\\Local\\Temp\\ipykernel_4804\\1040421081.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the tensorflow package\n",
    "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, epsilon = 1.0):\n",
    "        \"\"\"\"\n",
    "        # This constructor initializes the class with an exploration rate (epsilon), sets up the actions possible\n",
    "        # in the action space, creates a memory buffer, sets a discount rate (gamma), sets up an epsilon decay\n",
    "        # rate for adjusting exploration, sets a learning rate, and builds the deep learning model.\n",
    "\n",
    "        # Define the action space for the car race environment, with varying combinations of steering, acceleration, and brake\n",
    "\n",
    "        # action space Structure\n",
    "        #       (Steering Wheel, speed, Break)\n",
    "        # Range       -1-1       0-1   0-1\n",
    "\n",
    "        \"\"\"\n",
    "        self.action_space    = [(-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),\n",
    "                                (-1, 1,   0), (0, 1,   0), (1, 1,   0),\n",
    "                                (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),\n",
    "                                (-1, 0,   0), (0, 0,   0), (1, 0,   0)]\n",
    "        \n",
    "        self.memory          = deque(maxlen=5000) # Set up memory with a maximum size of 5000 for the agent's experiences\n",
    "        self.gamma           = 0.95 # Discount rate\n",
    "        self.epsilon         = epsilon # Exploration rate\n",
    "        self.epsilon_min     = 0.1\n",
    "        self.epsilon_decay   = 0.9999\n",
    "        self.learning_rate   = 0.001\n",
    "\n",
    "        # Create two instances of the model, one for predicting the Q-values, and another as the target model\n",
    "        # The target model is used to stabilize learning, by providing a fixed set of weights for calculating target values as seen in the Deep Q-Network paper\n",
    "        self.model           = self.build_model()\n",
    "        self.target_model    = self.build_model()\n",
    "        self.update_target_model()  # Copy weights from the model to the target model\n",
    "\n",
    "    def build_model(self):\n",
    "        # Define the deep learning model architecture - a CNN in this case (for visual tasks like the Box2D or Atari games)\n",
    "        # Model structure: Two convolutional layers, followed by flattening and dense layers\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=6, kernel_size=(7, 7), strides=3, activation='relu', input_shape=(96, 96, 3)))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(filters=12, kernel_size=(4, 4), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(216, activation='relu'))\n",
    "        model.add(Dense(len(self.action_space), activation=None))\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=self.learning_rate, epsilon=1e-7))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Function to copy the weights from the model to the target model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        # Function to store an experience in memory for experience replay\n",
    "        self.memory.append((state, self.action_space.index(action), reward, next_state, done))\n",
    "\n",
    "    def action(self, state):\n",
    "        # Function to select an action, either randomly (exploration) or the one with maximum predicted Q-value (exploitation)\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            # Get action with max value\n",
    "            action_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "            action_index = np.argmax(action_values[0])\n",
    "        else:\n",
    "            # Get random action\n",
    "            action_index = random.randrange(len(self.action_space))\n",
    "        return self.action_space[action_index]\n",
    "\n",
    "    def learn(self, batch_size):\n",
    "        # Function to update the model by replaying experiences from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        train_state = []\n",
    "        train_target = []\n",
    "        \n",
    "        # Get all predicted actions on all states\n",
    "        for state, action_index, reward, next_state, done in minibatch:\n",
    "            # For each experience, calculate the target Q-value\n",
    "            target = self.model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "            if done:\n",
    "                target[action_index] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(np.expand_dims(next_state, axis=0), verbose=0)[0]\n",
    "                target[action_index] = reward + self.gamma * np.amax(t)\n",
    "            # Save the input and output values\n",
    "            train_state.append(state)\n",
    "            train_target.append(target)\n",
    "        # Update the model using the states and target Q-values\n",
    "        self.model.fit(np.array(train_state), np.array(train_target), epochs=1, verbose=0)\n",
    "        \n",
    "        # Decay the epsilon value after each learning step\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        # Function to load weights into the model from a file, then copy them to the target model (for playing with trained weights)\n",
    "        self.model.load_weights(name)\n",
    "        self.update_target_model()\n",
    "\n",
    "    def save(self, name):\n",
    "        # Function to save the weights of the target model to a file (for later usage of the load function)\n",
    "        self.target_model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state_image(state):\n",
    "    \"\"\"\n",
    "    This function processes the input state (an image) by converting it to grayscale and normalizing the pixel values.\n",
    "    The purpose of this preprocessing step is to simplify the input without losing too much information.\n",
    "    \n",
    "    :param state: A 3-channel color image representing a game state.\n",
    "    :return: A grayscale image with normalized pixel values.\n",
    "    \"\"\"\n",
    "    \n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)    # Convert color image to grayscale\n",
    "    state = state.astype(float)                        # Convert data to float\n",
    "    state /= 255.0                                     # Normalize\n",
    "    return state\n",
    "\n",
    "def generate_state_frame_stack_from_queue(deque):\n",
    "    \"\"\"\n",
    "    A Function that takes a deque containing multiple frames of the game state and \n",
    "    converts it into a numpy array, then transposes the array to move the 'stack' dimension to the channel dimension. \n",
    "    This way, the stack of frames can be processed by a Convolutional Neural Network as separate channels.\n",
    "    \n",
    "    :param deque: A deque object containing multiple frames of the game state.\n",
    "    :return: A numpy array where the stack dimension has been moved to the channel dimension.\n",
    "    \"\"\"\n",
    "    frame_stack = np.array(deque)    \n",
    "    return np.transpose(frame_stack, (1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/100, Time Frames: 125, Total Rewards(adjusted): 9.1\n",
      "Episode: 2/100, Time Frames: 136, Total Rewards(adjusted): 2.9e+01\n",
      "Episode: 3/100, Time Frames: 195, Total Rewards(adjusted): 4.4e+01\n",
      "Episode: 4/100, Time Frames: 190, Total Rewards(adjusted): 5.4e+01\n",
      "Episode: 5/100, Time Frames: 58, Total Rewards(adjusted): -0.15\n",
      "5 Updated network\n",
      "Episode: 6/100, Time Frames: 125, Total Rewards(adjusted): 2.9e+01\n",
      "Episode: 7/100, Time Frames: 125, Total Rewards(adjusted): 2.8e+01\n",
      "Episode: 8/100, Time Frames: 125, Total Rewards(adjusted): 4.6e+01\n",
      "Episode: 9/100, Time Frames: 125, Total Rewards(adjusted): 1.9\n",
      "Episode: 10/100, Time Frames: 80, Total Rewards(adjusted): -0.18\n",
      "10 Updated network\n",
      "Episode: 11/100, Time Frames: 214, Total Rewards(adjusted): 5e+01\n",
      "Episode: 12/100, Time Frames: 94, Total Rewards(adjusted): -0.28\n",
      "Episode: 13/100, Time Frames: 125, Total Rewards(adjusted): 3.1e+01\n",
      "Episode: 14/100, Time Frames: 125, Total Rewards(adjusted): 1.5e+01\n",
      "Episode: 15/100, Time Frames: 141, Total Rewards(adjusted): 6.7e+01\n",
      "15 Updated network\n",
      "Episode: 16/100, Time Frames: 125, Total Rewards(adjusted): 1.7\n",
      "Episode: 17/100, Time Frames: 91, Total Rewards(adjusted): -0.45\n",
      "Episode: 18/100, Time Frames: 87, Total Rewards(adjusted): -0.079\n",
      "Episode: 19/100, Time Frames: 87, Total Rewards(adjusted): -0.29\n",
      "Episode: 20/100, Time Frames: 125, Total Rewards(adjusted): 3.3\n",
      "20 Updated network\n",
      "Episode: 21/100, Time Frames: 149, Total Rewards(adjusted): 7.3e+01\n",
      "Episode: 22/100, Time Frames: 119, Total Rewards(adjusted): -0.29\n",
      "Episode: 23/100, Time Frames: 156, Total Rewards(adjusted): 2e+01\n",
      "Episode: 24/100, Time Frames: 183, Total Rewards(adjusted): 8.9e+01\n",
      "Episode: 25/100, Time Frames: 136, Total Rewards(adjusted): 8.4e+01\n",
      "25 Updated network\n",
      "Episode: 26/100, Time Frames: 125, Total Rewards(adjusted): 5.4e+01\n",
      "Episode: 27/100, Time Frames: 132, Total Rewards(adjusted): 3.9e+01\n",
      "Episode: 28/100, Time Frames: 80, Total Rewards(adjusted): -0.22\n",
      "Episode: 29/100, Time Frames: 78, Total Rewards(adjusted): -0.03\n",
      "Episode: 30/100, Time Frames: 125, Total Rewards(adjusted): 5e+01\n",
      "30 Updated network\n",
      "Episode: 31/100, Time Frames: 122, Total Rewards(adjusted): -0.31\n",
      "Episode: 32/100, Time Frames: 125, Total Rewards(adjusted): 4.8\n",
      "Episode: 33/100, Time Frames: 125, Total Rewards(adjusted): 1.3e+01\n",
      "Episode: 34/100, Time Frames: 125, Total Rewards(adjusted): 7.4\n",
      "Episode: 35/100, Time Frames: 240, Total Rewards(adjusted): 8e+01\n",
      "35 Updated network\n",
      "Episode: 36/100, Time Frames: 79, Total Rewards(adjusted): -0.1\n",
      "Episode: 37/100, Time Frames: 125, Total Rewards(adjusted): 7.9\n",
      "Episode: 38/100, Time Frames: 125, Total Rewards(adjusted): 4.4e+01\n",
      "Episode: 39/100, Time Frames: 125, Total Rewards(adjusted): 3e+01\n",
      "Episode: 40/100, Time Frames: 64, Total Rewards(adjusted): -0.016\n",
      "40 Updated network\n",
      "Episode: 41/100, Time Frames: 116, Total Rewards(adjusted): -0.22\n",
      "Episode: 42/100, Time Frames: 125, Total Rewards(adjusted): 1.4e+01\n",
      "Episode: 43/100, Time Frames: 125, Total Rewards(adjusted): 1.1e+01\n",
      "Episode: 44/100, Time Frames: 125, Total Rewards(adjusted): 6.6\n",
      "Episode: 45/100, Time Frames: 100, Total Rewards(adjusted): -0.1\n",
      "45 Updated network\n",
      "Episode: 46/100, Time Frames: 82, Total Rewards(adjusted): -0.26\n",
      "Episode: 47/100, Time Frames: 147, Total Rewards(adjusted): 1.6e+01\n",
      "Episode: 48/100, Time Frames: 125, Total Rewards(adjusted): 3.8e+01\n",
      "Episode: 49/100, Time Frames: 121, Total Rewards(adjusted): -0.29\n",
      "Episode: 50/100, Time Frames: 125, Total Rewards(adjusted): 1.7e+01\n",
      "50 Updated network\n",
      "Episode: 51/100, Time Frames: 111, Total Rewards(adjusted): -0.19\n",
      "Episode: 52/100, Time Frames: 125, Total Rewards(adjusted): 6.8e+01\n",
      "Episode: 53/100, Time Frames: 180, Total Rewards(adjusted): 1.9e+01\n",
      "Episode: 54/100, Time Frames: 125, Total Rewards(adjusted): 2.3e+01\n",
      "Episode: 55/100, Time Frames: 125, Total Rewards(adjusted): 7.4\n",
      "55 Updated network\n",
      "Episode: 56/100, Time Frames: 125, Total Rewards(adjusted): 5.1e+01\n",
      "Episode: 57/100, Time Frames: 142, Total Rewards(adjusted): 4.6e+01\n",
      "Episode: 58/100, Time Frames: 195, Total Rewards(adjusted): 1.6e+02\n",
      "Episode: 59/100, Time Frames: 162, Total Rewards(adjusted): 6e+01\n",
      "Episode: 60/100, Time Frames: 125, Total Rewards(adjusted): 3.2e+01\n",
      "60 Updated network\n",
      "Episode: 61/100, Time Frames: 274, Total Rewards(adjusted): 1.2e+02\n",
      "Episode: 62/100, Time Frames: 239, Total Rewards(adjusted): 2.1e+02\n",
      "Episode: 63/100, Time Frames: 125, Total Rewards(adjusted): 4.6e+01\n",
      "Episode: 64/100, Time Frames: 125, Total Rewards(adjusted): 5.4e+01\n",
      "Episode: 65/100, Time Frames: 131, Total Rewards(adjusted): 1.7e+02\n",
      "65 Updated network\n",
      "Episode: 66/100, Time Frames: 135, Total Rewards(adjusted): 1.2e+02\n",
      "Episode: 67/100, Time Frames: 272, Total Rewards(adjusted): 1.6e+02\n",
      "Episode: 68/100, Time Frames: 177, Total Rewards(adjusted): 1.7e+02\n",
      "Episode: 69/100, Time Frames: 161, Total Rewards(adjusted): 1.1e+02\n",
      "Episode: 70/100, Time Frames: 125, Total Rewards(adjusted): 5e+01\n",
      "70 Updated network\n",
      "Episode: 71/100, Time Frames: 125, Total Rewards(adjusted): 5.9\n",
      "Episode: 72/100, Time Frames: 316, Total Rewards(adjusted): 1.6e+02\n",
      "Episode: 73/100, Time Frames: 125, Total Rewards(adjusted): 9.4e+01\n",
      "Episode: 74/100, Time Frames: 328, Total Rewards(adjusted): 2.8e+02\n",
      "Episode: 75/100, Time Frames: 125, Total Rewards(adjusted): 6.5e+01\n",
      "75 Updated network\n",
      "Episode: 76/100, Time Frames: 641, Total Rewards(adjusted): 8.8e+02\n",
      "Episode: 77/100, Time Frames: 125, Total Rewards(adjusted): 4.6e+01\n",
      "Episode: 78/100, Time Frames: 125, Total Rewards(adjusted): 1.3e+02\n",
      "Episode: 79/100, Time Frames: 166, Total Rewards(adjusted): 1.3e+02\n",
      "Episode: 80/100, Time Frames: 125, Total Rewards(adjusted): 1.4e+02\n",
      "80 Updated network\n",
      "Episode: 81/100, Time Frames: 125, Total Rewards(adjusted): 1.7e+02\n",
      "Episode: 82/100, Time Frames: 331, Total Rewards(adjusted): 5e+02\n",
      "Episode: 83/100, Time Frames: 509, Total Rewards(adjusted): 7.6e+02\n",
      "Episode: 84/100, Time Frames: 125, Total Rewards(adjusted): 1.1e+02\n",
      "Episode: 85/100, Time Frames: 125, Total Rewards(adjusted): 7.9e+01\n",
      "85 Updated network\n",
      "Episode: 86/100, Time Frames: 140, Total Rewards(adjusted): 1.3e+02\n",
      "Episode: 87/100, Time Frames: 204, Total Rewards(adjusted): 2.8e+02\n",
      "Episode: 88/100, Time Frames: 125, Total Rewards(adjusted): 7.2e+01\n",
      "Episode: 89/100, Time Frames: 125, Total Rewards(adjusted): 1.7e+02\n",
      "Episode: 90/100, Time Frames: 122, Total Rewards(adjusted): -0.18\n",
      "90 Updated network\n",
      "Episode: 91/100, Time Frames: 248, Total Rewards(adjusted): 3.5e+02\n",
      "Episode: 92/100, Time Frames: 141, Total Rewards(adjusted): 2.1e+02\n",
      "Episode: 93/100, Time Frames: 426, Total Rewards(adjusted): 1e+03\n",
      "Episode: 94/100, Time Frames: 518, Total Rewards(adjusted): 6.9e+02\n",
      "Episode: 95/100, Time Frames: 241, Total Rewards(adjusted): 4.1e+02\n",
      "95 Updated network\n",
      "Episode: 96/100, Time Frames: 154, Total Rewards(adjusted): 1.2e+02\n",
      "Episode: 97/100, Time Frames: 125, Total Rewards(adjusted): 1.4e+02\n",
      "Episode: 98/100, Time Frames: 316, Total Rewards(adjusted): 6.9e+02\n",
      "Episode: 99/100, Time Frames: 242, Total Rewards(adjusted): 3.2e+02\n",
      "Episode: 100/100, Time Frames: 289, Total Rewards(adjusted): 5.9e+02\n",
      "100 Updated network\n"
     ]
    }
   ],
   "source": [
    "# Paramters for training like the amount of episodes and frequency of timesteps to update the target network\n",
    "ENDING_EPISODE                = 100 \n",
    "SKIP_FRAMES                   = 2 \n",
    "TRAINING_BATCH_SIZE           = 64\n",
    "UPDATE_TARGET_MODEL_FREQUENCY = 5\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Create the gym environment and the agent for the CarRacing-v2 env.\n",
    "    env = gym.make('CarRacing-v2')\n",
    "    agent = DQNAgent()\n",
    "\n",
    "    # Train the agent through multiple episodes (up until the defined episode)\n",
    "    for episode in range(1, ENDING_EPISODE+1):\n",
    "\n",
    "        # Initialize values for each episode\n",
    "        init_state = env.reset()\n",
    "        init_state = process_state_image(init_state[0]) # Image processing for the network with the defined function\n",
    "\n",
    "        # Set variables for training\n",
    "        total_reward = 0\n",
    "        negative_reward_counter = 0\n",
    "        state_frame_stack_queue = deque([init_state] * 3, maxlen=3)\n",
    "        time_frame_counter = 1\n",
    "        done = False\n",
    "        \n",
    "        while True: # Continue until the episode ends\n",
    "            current_state_frame_stack = generate_state_frame_stack_from_queue(state_frame_stack_queue)\n",
    "            action = agent.action(current_state_frame_stack)\n",
    "            reward = 0\n",
    "\n",
    "            # Step the environment and get the reward\n",
    "            for each_frame in range(SKIP_FRAMES+1):\n",
    "                next_state, frame_reward, done, info, _ = env.step(action)\n",
    "                reward += frame_reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Terminate this episode if continuously getting negative rewards\n",
    "            if time_frame_counter > 100 and reward < 0:\n",
    "                negative_reward_counter = negative_reward_counter + 1\n",
    "            else:\n",
    "                negative_reward_counter = 0\n",
    "\n",
    "            # Optionally you can define bonusses for actions, in this case a bonus is given for using full gas\n",
    "            if action[1] == 1 and action[2] == 0:\n",
    "                reward *= 1.5\n",
    "\n",
    "            # Sum up reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # Save the next state\n",
    "            next_state = process_state_image(next_state)\n",
    "            state_frame_stack_queue.append(next_state)\n",
    "            next_state_frame_stack = generate_state_frame_stack_from_queue(state_frame_stack_queue)\n",
    "\n",
    "            # Memorize the agent's current state, action, reward and next state as one experience for experience replay\n",
    "            agent.memorize(current_state_frame_stack, action, reward, next_state_frame_stack, done)\n",
    "            \n",
    "            # Multiple conditions for stopping the episode, like a negative reward\n",
    "            if done or negative_reward_counter >= 25 or total_reward < 0:\n",
    "                print('Episode: {}/{}, Time Frames: {}, Total Rewards(adjusted): {:.2}'.format(episode, ENDING_EPISODE, time_frame_counter, float(total_reward)))\n",
    "                break\n",
    "\n",
    "            # Allow the model to learn from experience replay\n",
    "            if len(agent.memory) > TRAINING_BATCH_SIZE:\n",
    "                agent.learn(TRAINING_BATCH_SIZE)\n",
    "\n",
    "            time_frame_counter += 1\n",
    "\n",
    "        # Update the target model after the defined number of episodes\n",
    "        if episode % UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
    "            agent.update_target_model()\n",
    "            print(episode, \"Updated network\")\n",
    "\n",
    "        # Save the model after the last episode for future usage\n",
    "        if episode == ENDING_EPISODE:\n",
    "            agent.save('./model/trial_{}.h5'.format(episode))\n",
    "\n",
    "    env.close()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below opens the environment and shows how the trained model is playing the game. As it is possible for the model to have performance issues when exectued in a jupyter notebook, the same code is compressed into the run_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected action at step 1: (0, 0, 0.2)\n",
      "Selected action at step 2: (-1, 1, 0)\n",
      "Selected action at step 3: (-1, 1, 0)\n",
      "Selected action at step 4: (-1, 0, 0)\n",
      "Selected action at step 5: (1, 1, 0.2)\n",
      "Selected action at step 6: (-1, 1, 0)\n",
      "Selected action at step 7: (1, 0, 0.2)\n",
      "Selected action at step 8: (1, 1, 0.2)\n",
      "Selected action at step 9: (1, 0, 0)\n",
      "Selected action at step 10: (1, 0, 0)\n",
      "Selected action at step 11: (1, 0, 0)\n",
      "Selected action at step 12: (1, 0, 0)\n",
      "Selected action at step 13: (-1, 1, 0)\n",
      "Selected action at step 14: (-1, 1, 0)\n",
      "Selected action at step 15: (-1, 1, 0)\n",
      "Selected action at step 16: (-1, 1, 0)\n",
      "Selected action at step 17: (-1, 1, 0)\n",
      "Selected action at step 18: (-1, 1, 0)\n",
      "Selected action at step 19: (0, 0, 0.2)\n",
      "Selected action at step 20: (0, 1, 0.2)\n",
      "Selected action at step 21: (-1, 1, 0)\n",
      "Selected action at step 22: (1, 1, 0)\n",
      "Selected action at step 23: (0, 1, 0)\n",
      "Selected action at step 24: (1, 1, 0)\n",
      "Selected action at step 25: (0, 1, 0)\n",
      "Selected action at step 26: (1, 1, 0)\n",
      "Selected action at step 27: (0, 1, 0)\n",
      "Selected action at step 28: (1, 1, 0)\n",
      "Selected action at step 29: (1, 1, 0)\n",
      "Selected action at step 30: (1, 1, 0)\n",
      "Selected action at step 31: (1, 1, 0)\n",
      "Selected action at step 32: (1, 1, 0)\n",
      "Selected action at step 33: (1, 1, 0)\n",
      "Selected action at step 34: (1, 1, 0.2)\n",
      "Selected action at step 35: (0, 0, 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m         time_frame_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     41\u001b[0m     env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m---> 43\u001b[0m run_game()\n",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m, in \u001b[0;36mrun_game\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m env\u001b[39m.\u001b[39mrender()\n\u001b[0;32m     22\u001b[0m current_state_frame_stack \u001b[39m=\u001b[39m generate_state_frame_stack_from_queue(state_frame_stack_queue)\n\u001b[1;32m---> 23\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49maction(current_state_frame_stack)\n\u001b[0;32m     24\u001b[0m next_state, reward, done, info, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action) \u001b[39m# Take a step in the environment using the selected action\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m# Print the selected action\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 59\u001b[0m, in \u001b[0;36mDQNAgent.action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maction\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Function to select an action, either randomly (exploration) or the one with maximum predicted Q-value (exploitation)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[0;32m     58\u001b[0m         \u001b[39m# Get action with max value\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m         action_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49mexpand_dims(state, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     60\u001b[0m         action_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(action_values[\u001b[39m0\u001b[39m])\n\u001b[0;32m     61\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[39m# Get random action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Valentin\\.virtualenvs\\RL-CarRacing-v2-ROZ9WfjW\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1696\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1690\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mUsing Model.predict with \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1692\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1693\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1694\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1696\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1697\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1698\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1699\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   1700\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   1701\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1702\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1703\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1704\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1705\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1706\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1708\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\Valentin\\.virtualenvs\\RL-CarRacing-v2-ROZ9WfjW\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1364\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1363\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1364\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Valentin\\.virtualenvs\\RL-CarRacing-v2-ROZ9WfjW\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1154\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1152\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m   1153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verify_data_adapter_compatibility(adapter_cls)\n\u001b[1;32m-> 1154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1155\u001b[0m     x,\n\u001b[0;32m   1156\u001b[0m     y,\n\u001b[0;32m   1157\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1158\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1159\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1160\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1161\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1162\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1163\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1164\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1165\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mds_context\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1166\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1168\u001b[0m strategy \u001b[39m=\u001b[39m ds_context\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Valentin\\.virtualenvs\\RL-CarRacing-v2-ROZ9WfjW\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:337\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m     flat_dataset \u001b[39m=\u001b[39m flat_dataset\u001b[39m.\u001b[39mshuffle(\u001b[39m1024\u001b[39m)\u001b[39m.\u001b[39mrepeat(epochs)\n\u001b[0;32m    335\u001b[0m   \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m--> 337\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mflat_map(slice_batch_indices)\n\u001b[0;32m    339\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    341\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Valentin\\.virtualenvs\\RL-CarRacing-v2-ROZ9WfjW\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1957\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1934\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_map\u001b[39m(\u001b[39mself\u001b[39m, map_func):\n\u001b[0;32m   1935\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[0;32m   1936\u001b[0m \n\u001b[0;32m   1937\u001b[0m \u001b[39m  Use `flat_map` if you want to make sure that the order of your dataset\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1955\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   1956\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1957\u001b[0m   \u001b[39mreturn\u001b[39;00m FlatMapDataset(\u001b[39mself\u001b[39;49m, map_func)\n",
      "File \u001b[1;32mc:\\Users\\Valentin\\.virtualenvs\\RL-CarRacing-v2-ROZ9WfjW\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4571\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4567\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   4568\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`map_func` must return a `Dataset` object. Got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   4569\u001b[0m           \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure)))\n\u001b[0;32m   4570\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure\u001b[39m.\u001b[39m_element_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 4571\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49mflat_map_dataset(\n\u001b[0;32m   4572\u001b[0m     input_dataset\u001b[39m.\u001b[39;49m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   4573\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_func\u001b[39m.\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs,\n\u001b[0;32m   4574\u001b[0m     f\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_func\u001b[39m.\u001b[39;49mfunction,\n\u001b[0;32m   4575\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_structure)\n\u001b[0;32m   4576\u001b[0m \u001b[39msuper\u001b[39m(FlatMapDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Valentin\\.virtualenvs\\RL-CarRacing-v2-ROZ9WfjW\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:2096\u001b[0m, in \u001b[0;36mflat_map_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2094\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   2095\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2096\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   2097\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mFlatMapDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_dataset, other_arguments, \u001b[39m\"\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m, f,\n\u001b[0;32m   2098\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types, \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[0;32m   2099\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   2100\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Specify the saved model to load\n",
    "MODEL = 'model/trial_100.h5'\n",
    "\n",
    "def run_game():\n",
    "    env = gym.make('CarRacing-v2', render_mode=\"human\") # Create the gym environment and the agent for the CarRacing-v2 env.\n",
    "    agent = DQNAgent(epsilon=0) # Epsilon 0 allows deterministic action selection\n",
    "    agent.load(MODEL) # load the pre-trained model\n",
    "\n",
    "    # Initialize the state of the environment and process the initial image\n",
    "    init_state = env.reset()\n",
    "    init_state = process_state_image(init_state[0])\n",
    "\n",
    "    total_reward = 0\n",
    "    state_frame_stack_queue = deque([init_state] * 3, maxlen=3)\n",
    "    time_frame_counter = 1\n",
    "    \n",
    "    # Game loop, this renders it and executes the actions given\n",
    "    while True:\n",
    "        # Render the environment on screen\n",
    "        env.render()\n",
    "\n",
    "        current_state_frame_stack = generate_state_frame_stack_from_queue(state_frame_stack_queue)\n",
    "        action = agent.action(current_state_frame_stack)\n",
    "        next_state, reward, done, info, _ = env.step(action) # Take a step in the environment using the selected action\n",
    "        \n",
    "        # Print the selected action\n",
    "        print(f\"Selected action at step {time_frame_counter}: {action}\")\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        # Process the image of the next state and add it to the deque\n",
    "        next_state = process_state_image(next_state)\n",
    "        state_frame_stack_queue.append(next_state)\n",
    "   \n",
    "        # If the game is done, print the total frames and reward, then break the loop (as seen in the training)\n",
    "        if done:\n",
    "            print('Time Frames: {}, Total Rewards: {:.2}'.format(time_frame_counter, float(total_reward)))\n",
    "            break\n",
    "        time_frame_counter += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "run_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The findings from training the RL agent in the CarRacing environment across different numbers\n",
    "of episodes reveal a progressive improvement in performance and driving style. The agent was\n",
    "trained for 5, 10, 100 and 500 episodes.\n",
    "\n",
    "After 5 episodes, the agent exhibits rudimentary control over the car. Its strategy at this stage\n",
    "primarily involves braking and slowly moving forward, gradually accumulating rewards but offering\n",
    "little in the way of strategic navigation.\n",
    "\n",
    "Increasing the training duration to 10 episodes, the agent begins to show the first signs of a more\n",
    "purposeful driving direction. While the driving pattern at this stage is still far from optimal, as it\n",
    "quickly leads to mistakes being made and thus failing to run continuously, there’s an improvement\n",
    "over the initial strategy.\n",
    "\n",
    "The model with 100 episodes already shows great results. It seems as a minimum number of\n",
    "episodes is required in order for the agent to gather the basic techniques of driving. However the\n",
    "model does not seem as stable, when going around corners the actions taken often lead to minor\n",
    "setbacks and the car getting of the track. When the car is of the track, it has a hard time going\n",
    "into the correct direction and can get stuck quickly.\n",
    "\n",
    "The leap to 500 episodes is another leap the agent’s performance. It displays a proper driving\n",
    "style, smoothly navigating the track while minimizing off-road detours. The improvement can be\n",
    "seen mainly in the stable driving style, while the reward only improves slightly. However, there\n",
    "remain a few areas for improvement. In certain iterations and specific track layouts, the agent\n",
    "exhibits a tendency to cut corners. This results in the car sometimes ending up facing the wrong\n",
    "direction. Nonetheless, the overall performance at this stage is superior to earlier stages, as it\n",
    "often finds its way back to the road and is able to recover minor mishaps.\n",
    "\n",
    "The maximum total reward of each model after trying out five runs each was manually observed\n",
    "and is visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Manually observed highest total reward \n",
    "height = [17, 61, 849, 910]\n",
    "bars = ('5 Episodes', '10 Episodes', '100 Episodes', '500 Episodes')\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "plt.bar(y_pos, height)\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.ylabel('Max. Total Reward')\n",
    "plt.title('Max Total Reward by amount of episodes')\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnings and challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the span of the one of the key areas where significant learning took place was during the implementation of the \\ac{dqn}. It helped understanding the theoretical framework behind reinforcement learning and deep learning.\n",
    " \n",
    "Alongside this rewarding learning experience, there were some challenges encountered, especially with software dependencies. I faced issues with installing SWIG for the Box2D environments, which was resolved by installing SWIG manually. Also, finding the correct version of Tensorflow that can make use of my GPU. This necessitated research and deep dive into the dependencies of these libraries, their compatibility with other tools. In the process, it seemed that my CUDA version was not supported, forcing a downgrade from CUDA 12.2 to 11.2. These challenges broadened my understanding of python and virtual environments in python and were part of the learning experience.\n",
    "\n",
    "In summary, this project was a perfect blend of theory and practice, with significant learnings and challenges that have greatly enriched my understanding of deep learning, reinforcement learning, and dependency management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
